---
title:  "[ML이론] 학습률과 학습률 스케줄링"
excerpt: "Machine Learning 이론 정리(3): 학습률(learning rate) 정리"

categories:
  - AI
  - Machine Learning
tags:
  - Machine Learning
  - Deep Learning

last_modified_at: 2022-08-28T10:00:00-05:00

published: false
---

# 학습률

## 학습률과 수렴의 관계
학습률이 너무 크면 훈련이 발산하고, 적당히 크면 처음에는 빨리 수렴하다 최적점 근처에서 진동이 심해져 수렴이 안될 수도 있다. 반대로 학습률이 너무 작으면 최적점에 수렴은 하지만 시간이 오래 걸린다.

특정 고정된 학습률로 여러 번 훈련을 해보는 것보다는 조금 더 효율적 방법이 있다. 큰 학습률로 학습을 시작해 빠르게 수렴시키고 학습 속도가 느려질 때 학습률을 낮춰 최적점에 진동 없이 수렴시키는 것이다. 관련한 내용은 [학습 스케줄링](#학습-스케줄링)에서 다룬다.

- - -

# 학습 스케줄링
학습 스케줄링(learning scheduling) 혹은 학습 스케줄은 모델을 훈련시키는 동안 학습률을 감소시키는 방식이다. 아래는 간단한 notation을 정리했다.

| notation | description |
|:---:|:---:|
| $\eta$, $\eta_0$ | 학습률, 초기 학습률 |
| $t$ | 반복 횟수 |
| $s$ | 스텝 횟수 |

## 거듭제곱(power) 기반 스케줄링

$$
\eta(t) = \cfrac{\eta_0}{(1+t/s)^c}
$$

매 스텝 $s$ 마다 학습률을 감소시킨다. 보통 거듭제곱수인 $c$ 는 1로 지정하여, $s$ 스텝 뒤에는 학습률이 $\eta_0 / 2$ 가 되고 $2s$ 스텝 뒤에는 $\eta_0 / 3$ 이 될 것이다. 따라서 학습률은 처음엔 빠르게 감소하다 점차 느려질 것이다.

## 지수(exponential) 기반 스케줄링

$$
\eta(t) = \eta_0 0.1^{(t/s)}
$$

학습률을 매 스텝 $s$ 마다 10배씩 줄인다. 시간이 갈수록 천천히 줄이는 것이 아닌 계속 10배씩 줄이는 데 거듭제곱 기반 방식과 차이가 있다.

## 구간별 고정(piecewise constant) 스케줄링
몇 번의 에폭은 일정한 학습률로, 그 다음 몇 번의 에폭은 더 작은 일정한 학습률로 학습시키는 방식이다. 최적의 학습률과 에폭 수의 조합을 찾는 것이 필요하다.

## 성능 기반(performance) 스케줄링
특정 몇 번의 스텝마다 검증 오차를 측정하여, 오차가 줄어들이 않았다면 $\lambda$ 배만큼 학습률을 감소시킨다.

## 1사이클(1cycle) 스케줄링
훈련 절반은 학습률을 $\eta_0 \rightarrow \eta_1$ 로 선형적 증가시키고, 다음 절반은 $\eta_1 \rightarrow \eta_0$ 로 다시 선형적 감소시킨다. 마지막 몇 번의 에폭은 소수점 몇 째 자리까지 학습률을 선형적으로 줄인다.<br>
최대 학습률 $\eta_1$ 은 최적 학습률을 찾을 때처럼 선택해야 하는 값이며, 초기 학습률 $\eta_0$ 은 $\eta_1$ 의 약 0.1배 정도로 한다.

모멘텀도 비슷한 방식으로 사용한다. 처음엔 높은 모멘텀에서 낮은 모멘텀으로 선형적 감소시키며 학습의 절반을, 다음 절반은 다시 높은 모멘텀으로 선형적 증가시키고, 마지막 몇 에포크는 최댓값으로 한다.

> [논문 보기](https://arxiv.org/pdf/1803.09820.pdf) Smith, Leslie N. "**A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay**." arXiv preprint arXiv:1803.09820 (2018).

- - -

# References
* 핸즈온 머신러닝
