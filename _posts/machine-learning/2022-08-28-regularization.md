---
title:  "[ML이론] 학습률과 학습률 스케줄링"
excerpt: "Machine Learning 이론 정리(3): 학습률(learning rate) 정리"

categories:
  - Machine Learning
tags:
  - Machine Learning
  - Deep Learning

last_modified_at: 2022-08-28T11:05:00-05:00

published: false
---

# 규제
모델이 많은 파라미터를 가지고 있다면 대규모의 복잡한 데이터셋을 잘 학습할 수 있으나, 훈련 데이터셋에 과대적합될 위험도 함께 증가한다. 과대적합을 피하기 위하여 규제(regularization)을 도입한다.

* 조기 종료
* 배치 정규화
* l1, l2 규제
* 드롭아웃
* 맥스-노름 규제

# 조기 종료
TODO 핸즈온 머신러닝 10장

# 배치 정규화
불안정한 그래디언트 문제를 해결하기 위하여 고안되었다. 하지만 규제에도 곧잘 쓰인다.

# l1 & l2 규제
TODO 핸즈온 머신러닝 4장 

$\ell_1$ 규제는 희소 모델을 만들 때, $\ell_2$ 규제는 신경망의 연결 가중치를 제한할 때 쓰인다.

# 드롭아웃
드롭아웃(dropout)은 매 훈련 스텝마다 각 뉴런이 $p$ 의 확률로 무시되는 방식이다. 이때 입력 뉴런은 포함하나 출력 뉴런은 제외한다. 일반적으로는 출력층을 제외하고 맨위 ~ 세 번째 층까지만 적용한다. 훈련이 끝나면 뉴런에 드롭아웃을 적용하지 않는다.

$p$ 는 드룹아웃 비율(dropout rate)로, 보통 10~50% 사이 값을 지정한다. 순환 신경망에서는 주로 20~30%를, 합성곱 신경망에서는 주로 40~50%을 사용한다. <br>
$p = 50\%$ 로 지정할 경우, 테스트 시 하나의 뉴런이 훈련 시보다 약 두 배 정도 많은 입력 뉴런과 연결되어 두 배 많은 신호를 받기 때문에 잘 동작하지 않을 수 있다. 이를 보상하기 위해 훈련 종료 후 각 뉴런의 가중치에 0.5를 곱한다(보존 확률(keep probability) $(1-p)$를 곱한다고 표현하기도 한다).

드롭아웃은 네트워크를 안정시키고 일반화 성능을 좋게 한다.<br>훈련 시 뉴런이 이웃한 뉴런에 적응하지 않게 한다. 몇 개의 입력 뉴런에만 의존하지 않고 모든 입력 뉴런에 집중함으로써 입력값의 작은 변화에 덜 민감해진다.

드롭아웃은 각 훈련 스텝마다 네트워크가 새로 생성된다고 볼 수도 있다. 드롭아웃이 가능한 뉴런이 N개 있다고 하면 $2^N$ 개의 네트워크가 가능한 셈이기 때문이다. 드롭아웃을 통해 만들어진 신경망은 $2^N$ 개의 신경망을 평균한 앙상블로 볼 수 있는 것이다.

주의해야 할 것은, 드롭아웃은 훈련 시에만 활성화시키고 훈련 손실 평가, 검증 손실 평가 시에는 드롭아웃을 제외해야 한다.

모델이 훈련 데이터셋에 과대적합 되었을 때, 층이 클 때는 드롭아웃의 비율을 늘려본다. 최근에는 마지막 은닉층 뒤에만 드롭아웃을 사용하기도 한다.

드롭아웃 중에는 몬테 카를로 드롭아웃(Monte Carlo dropout, MC 드롭아웃)도 있다. 드롭아웃으로 만든 예측을 평균하는 방식이다. 훈련된 드롭아웃 모델을 재훈련하거나 수정하지 않고도 성능을 향상시킬 수 있으며, 모델의 불확실성을 더 잘 추정할 수 있다고 한다.<br>

# 맥스-노름 규제
맥스-노름(max-norm) 규제는 불안정한 기울기 문제를 완화하는 데 도움을 준다. 해당 규제에서는 전체 손실 함수에 규제 손실항을 추가하지 않고, 매 훈련 스텝 종료 후 각 뉴런의 입력 연결 가중치($\mathbf{w}$)가 (1)과 같도록 제한하고, 이를 벗어난다면 (2)의 식으로 $\mathbf{w}$ 를 조정한다.

$$
\begin{align}
&\lVert \mathbf{w} \rVert_2 \leq r \\
&\mathbf{w} \leftarrow \mathbf{w} \cfrac{r}{\lVert \mathbf{w} \rVert_2}
\end{align}
$$

맥스-노름 규제의 하이퍼 파라미터인 $r$ 을 작게 설정하면 규제의 양이 증가해 과대 적합을 감소시킨다.
