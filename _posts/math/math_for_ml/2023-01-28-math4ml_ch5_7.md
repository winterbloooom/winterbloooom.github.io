---
title:  "Mathematics for Machine Learning 5.7장: 벡터 연산 - 고차 미분 (헤시안 행렬)"
excerpt: "Vector Calculus - Higher-Order Derivatives (Hessian)"

categories:
  - Math
  - Math for ML
tags:
  - Math
  - Machine Learning

last_modified_at: 2023-01-28
---

{% include inserted_box.html text="이 내용은 책 <a href='https://mml-book.github.io/book/mml-book.pdf'>Mathematics for machine learning(Marc Peter Deisenroth et al.)</a>을 기반으로 하고 있습니다." %}

# 5.7: Higher-Order Derivatives

이전까지는 그래디언트, 즉 일차 미분만을 다뤘다. 이번 장에서는 이차 이상, 즉 고차 미분을 다룬다. ('고차'와 '다변수'를 구별할 것!) 7장에서 보겠지만, 최적화에 사용되는 Newton's method(뉴턴법)에서는 이차 미분을 사용한다.

앞으로 사용할 몇 가지 표기법(notation)을 보자. 두 변수 $x, y$의 함수 $f: \mathbb{R}^2 \rightarrow \mathbb{R}$에 대하여

* $\cfrac{\partial ^n f}{\partial x^n}$  : $x$에 대한 $f$의 $n$차 편미분
* $\cfrac{\partial ^2 f}{\partial y \partial x} = \cfrac{\partial}{\partial y} \left( \cfrac{\partial f}{\partial x} \right)$  : $x$에 대한 $f$의 편미분을 다시 $y$에 대해 편미분한 것.

함수 $f(x, y)$ 가 두번 미분이 가능한 함수라면, $\cfrac{\partial ^2 f}{\partial y \partial x} = \cfrac{\partial ^2 f}{\partial x \partial y}$ , 즉 미분의 순서를 바꿔도 결괏값은 동일하다.


## Hessian(헤시안)

<div id="def-box">
<div class="def-title">헤시안 (Hessian)</div>
<p markdown="1">

헤시안 $\triangledown_{x, y}^2 f(x, y)$ 은 이차 편미분 결과를 모은 것이다. $\boldsymbol{x} \in \mathbb{R}^n$ 과 $f: \mathbb{R}^n \rightarrow \mathbb{R}$ 에 대해 헤시안은 $n \time n$ 행렬의 형태를 갖는다. 
</p>
</div>

덧붙여, 벡터장 $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 에 대해 헤시안은 $m \times n \times n$ 형태의 텐서이다. 헤시안은 $(x, y)$ 근처에서 함수의 곡률(curvature)을 나타낸다.

헤시안 행렬(Hessian matrix)는 아래와 같다. $\cfrac{\partial ^2 f}{\partial y \partial x} = \cfrac{\partial ^2 f}{\partial x \partial y}$ 이므로 헤시안 행렬은 대칭행렬(symmetric mat)이다.

$$
\boldsymbol{H} = \begin{bmatrix}
  \cfrac{\partial ^2 f}{\partial x^2} & \cfrac{\partial ^2 f}{\partial y \partial x} \\
  \cfrac{\partial ^2 f}{\partial x \partial y} & \cfrac{\partial ^2 f}{\partial y^2}
\end{bmatrix}
$$