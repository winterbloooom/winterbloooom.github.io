---
title:  "Mathematics for Machine Learning 6.3장: 합/곱의 규칙, 베이즈 정리"
excerpt: "Probability and Distributions - Sum Rule, Product Rule, and Bayes' Theorem"

categories:
  - Math
  - Math for ML
tags:
  - Math
  - Machine Learning

last_modified_at: 2023-02-04
---

{% include inserted_box.html text="이 내용은 책 <a href='https://mml-book.github.io/book/mml-book.pdf'>Mathematics for machine learning(Marc Peter Deisenroth et al.)</a>을 기반으로 하고 있습니다." %}


# 6.3. Sum Rule, Product Rule, and Bayes' Theorem

## Sum Rule

확률 이론에서 합의 규칙(sum rule)이란, 어떤 확률변수의 상태들의 집합을 더하면(적분하면) 확률이 된다는 것이다.

$$
p(\boldsymbol{x}) = 
\begin{cases}
  \sum \limits_{\boldsymbol{y} \in \mathcal{y}} p(\boldsymbol{x, y}) & \mathrm{if} \ \boldsymbol{y} \ \mathrm{is \ discrete} \\
  \int_{\mathcal{y}} p(\boldsymbol{x, y}) d \boldsymbol{y} & \mathrm{if} \ \boldsymbol{y} \ \mathrm{is \ continuous}
\end{cases} \tag{1}
$$

위 식에서 $\mathcal{y}$ 는 확률변수 Y의 target space의 상태들을 말한다.

합의 규칙을 다른 말로는 'marginalization property'라고도 부른다. 주변 확률을 합으로 나타낼 수 있기 때문이다. 이는 아래의 설명에서 이해할 수 있다.

합의 규칙은 결합 분포를 주변 분포와 연결시키기도 한다. 2종류 이상의 확률 변수를 가진 결합 분포가 있다 할 때, 합의 규칙은 어떤 확률 변수의 부분집합에도 적용될 수 있다. 그 결과가 하나 이상의 확률 변수의 주변 분포가 된다. 식으로 표현하자면, $\boldsymbol{x} = [x_1, \cdots, x_D]^T$ 가 있을 때 marginal(주변 확률)은 아래와 같이 얻는다. 즉, $x_i$를 제외하고 모든 확률 변수에 대해 더하는 합의 규칙을 반복적으로 적용한다.

$$
p(x_i) = \int p(x_1, \cdots, x_D) d\boldsymbol{x}_{\backslash i}
$$

{% include inserted_box.html text="$\backslash i$ 는 $i$ 를 제외하고 전부라는 뜻이다. (all except $i$ )" %}

조금 더 친숙한 형태로 바꿔보자면, 주변 분포의 정의 상, 확률 변수 X와 Y가 있을 때 $X=x_i$ 일 확률은 가능한 모든 Y의 경우의 수 $Y=y_1, \cdots, y_L$ 를 다 합한 형태다.

$$
p(X=x_i) = \sum \limits_{j=1}^L p(X=x_i, Y=y_i)
$$

좌변은 주변확률인데 우변은 결합 확률이다. 그래서 주변 분포와 결합 분포를 연결한다고 표현한 것이다.

## Product Rule

합의 규칙이 '주변 분포'와 결합 분포를 연결시켰다면, 곱의 규칙은 '조건부 분포'와 결합 분포를 연결시킨다.

$$
\begin{aligned}
  p(\boldsymbol{x, y}) = &p(\boldsymbol{y \vert x}) p(\boldsymbol{x}) = p(\boldsymbol{x \vert y}) p(\boldsymbol{y}) \\
  &\left( \because p(\boldsymbol{y \vert x}) p(\boldsymbol{x}) = \cfrac{p(\boldsymbol{x, y})}{p(\boldsymbol{x})} p(\boldsymbol{x}) \right)
\end{aligned} \tag{2} 
$$

따라서, 두 확률 변수의 결합 분포 $p(\boldsymbol{x, y})$ 는 서로 다른 두 분포 $p(\boldsymbol{y \vert x}) \ , \ p(\boldsymbol{x})$ 의 곱으로 분해될 수 있다. 하나는 첫 번째 확률 변수의 주변 분포 $p(\boldsymbol{x})$ 이고, 다른 하나는 첫 번째 확률변수가 주어졌을 때 두 번째 확률변수의 조건부 분포 $p(\boldsymbol{y \vert x})$ 이다. 결합 분포는 확률 변수의 순서에 관계가 없으니 위처럼 두 가지의 분해 경우가 나타난다.

## Bayes' Therom

### 바탕 지식

사과 한 알이 있고, 사과를 딴 곳이 영주, 안동, 청송, 예천, 안동, 문경 등이 있다고 하자. 아래와 같은 두 가지 조건부 확률을 고려해본다.

|사후 확률(posterior)|우도(likelihood, 가능도)|
|---|---|
|$P(안동\vert 사과)$|$P(사과 \vert 안동)$|
| 사과를 딴 곳이 안동일 확률|안동에서 이 사과를 땄을 확률|

사과가 어디서 재배되었을지 알아내기 위해서 최대 우도 추정(Maximum Likehood Estimation, MLE) 혹은 최대 사후 확률(Maximum A Posteriori, MAP) 추정 방식을 사용할 수 있다.

* 최대 우도 추정(Maximum Likehood Estimation, MLE)
  * 말 그대로 우도가 최대인 것을 찾는다.
  * $P(사과 \vert 안동), P(사과 \vert 영주), P(사과 \vert 청송), \cdots$ 등 가능한 모든 원산지에 대해 거기서 이 사과를 땄을 확률 모두를 구하고, 그 중에서 최대 우도를 갖는 원산지를 택한다.
* 최대 사후 확률(Maximum A Posteriori, MAP)
  * 말 그대로 사후 확률이 가장 높은 것을 찾는다.
  * $P(안동\vert 사과), P(영주\vert 사과), P(청송\vert 사과), \cdots$ 등 이 사과를 땄을 모든 원산지의 확률을 구하고, 그 중에서 최대 사후 확률을 갖는 원산지를 택한다.

베이즈 정리는 사후 확률 $P(안동\vert 사과)$ 를 구할 수 있게 한다.

{% include inserted_box.html text="이런 문제를 풀 때 둘 중에 정확한 방법을 꼽자면 최대 사후 확률이라고 한다. 사과를 두고 그게 어디서 난 건지를 맞추는 게 문제의 본질에 가깝기 때문이다. 상식적으로. 더구나 최대 우도 추정의 경우 모든 사전 확률이 같다는, 즉 $P(안동)=P(영주)=P(청송)=\cdots$ 라서 모든 지역에서 사과가 동일하게 생산된다거나 품질이나 모양이 다 같다는 가정 하에 근사적 해를 구하는 느낌이다." %}

{% include inserted_box.html text="머신러닝의 예로 들어보자면, '이 사진의 물체'(아까 전의 [사과])가 '컵'/'접시'/'물병'(아까 전의 [안동, 청송, 영주])일 확률이다." %}

### 수식

머신러닝과 베이즈 통계학에서는, 우리가 관찰해온 어떤 확률 변수들이 주어졌을 때, 아직 관찰하지 않은(latent) 확률 변수들을 추론하고 싶다.

베이즈 정리(규칙, 법칙)는 아래와 같은 식이다.

$$
\underbrace{p(\boldsymbol{x \vert y})}_{\mathrm{posterior}} = \cfrac{\overbrace{p(\boldsymbol{y \vert x})}^{\mathrm{likelihood}} \ \overbrace{p(\boldsymbol{x})}^{\mathrm{prior}}} {\underbrace{p(\boldsymbol{y})}_{\mathrm{evidence}}}
\tag{3}
$$

위 베이즈 정리 식(3)은 곱의 규칙 식(2)에서 만들어낼 수 있다.

$$
p(\boldsymbol{y \vert x}) p(\boldsymbol{x}) = p(\boldsymbol{x \vert y}) p(\boldsymbol{y})
\iff p(\boldsymbol{y \vert x}) = \cfrac{p(\boldsymbol{x \vert y}) p(\boldsymbol{y})}{p(\boldsymbol{x})}
$$

식 자체가 아닌 의미론적으로 다가가보자.

관측하지 않은 확률변수 $\boldsymbol{x}$ 에 대한 사전 지식 $p(\boldsymbol{x})$ ('prior')가 있고, 또다른 확률변수 $y$와의 관계 $p(\boldsymbol{y \vert x})$ ('likelihood')를 알고 있다고 하자. 이는 관측할 수 있는 값이다. 이 상태에서 $\boldsymbol{y}$ ('evidence')를 관측하고, 베이즈 정리를 통해 $\boldsymbol{y}$ 의 관측값이 주어졌을 때의 $\boldsymbol{x}$에 대한 결론 $p(\boldsymbol{x \vert y})$ ('posterior') 을 이끌어낼 수 있다.

말이 어려우니 아까 사과의 원산지 예시를 다시 사용하겠다. 위 베이즈 정리 식(3)에 따르면

$$
P(안동\vert 사과) = \cfrac{P(사과 \vert 안동) \ P(안동)}{P(사과)}
$$

이다. 

### Posterior term

사후확률 $P(안동\vert 사과)$ 는 이 사과가 수확되었는데(관측, 사건 발생) 얘가 특정 지역인 '안동'에서 생산되었을 확률이다. 머신러닝에서 보자면 관측이 된(발생이 된) '이 사건'이 '이 특정 모델/시스템'에서 발생했을 확률이다.

교재에 따른 표현으로는, $\boldsymbol{y}$ 라는 관측을 했고, 그 때 $\boldsymbol{x}$ 에 대해 무엇을 알고 있는지를 나타낸다. 베이즈 통계학에서 관심을 가지는 양이다.

### Prior term

$P(안동), P(청송), \cdots$ 는 각 지역 고유의 확률값이다. 사과를 고려하기 전에, 각 지역에 대해 가지고 있는 사전 지식이다. 각 지역에서 사과 생산량이 얼마나 되는지 등을 예로 들 수 있겠다. 머신러닝에서 보자면 시스템/모델(여러 '클래스(라벨)'의 집합이라고 보면 쉽다)에 대한 사전 확률로, 남녀의 구성비($P(남자), P(여자)$) 라던지 연령 분포($P(노인), P(어린이)$) 등이 되겠다.

교재에 따른 표현으로는, Prior인 $p(\boldsymbol{x})$는 어느 데이터를 관측하기 전에 관측하지 않은(latent) 확률변수 $\boldsymbol{x}$ 의 주관적(subjective) 사전 지식(prior knowledge)을 요약하고 있다. 모든 $\boldsymbol{x}$ 에 대해 pdf, pmf가 0이 아니기만 하면 그 어떤 prior도 선택 가능하다.

### Likelihood term

앞선 사과의 예에서 보았듯이, 우도인 $P(사과 \vert 안동)$ 은 안동에서 이 사과가 나올 확률이다. 머신러닝에서 보자면 이 모델에서 이 데이터(관측값)가 나올 확률이다.

교재에 따르면, Likelihood(우도, 또는 measurement model)인 $p(\boldsymbol{y \vert x})$ 는 $\boldsymbol{x, y}$가 어떻게 관련되어 있는지를 말한다. 우리가 잠재 변수(latent variable) $\boldsymbol{x}$ 를 알고 있다면(prior), 데이터 $\boldsymbol{y}$ 의 확률이 $p(\boldsymbol{y \vert x})$ 이다. $p(\boldsymbol{y \vert x})$ 는 $\boldsymbol{y}$ 에서의 분포이지, $\boldsymbol{x}$ 에서의 분포는 아니다. 

읽을 때는 '$\boldsymbol{x}$ 가 주어졌을 때의 $\boldsymbol{y}$ 의 확률' 혹은 '$\boldsymbol{y}$ 가 주어졌을 때의 $\boldsymbol{x}$ 의 우도' 라고 한다. 


### Evidence term

P(사과) 는 고정된 상수값(양, quantity)이다. Evidence 혹은 주변 가능도(marginal likelihood) $p(\boldsymbol{y})$ 라 하며, 아래와 같이 계산할 수 있다.

$$
\begin{aligned}
  p(\boldsymbol{y}) 
  := &\int p(\boldsymbol{y \vert x}) p(\boldsymbol{x}) d\boldsymbol{x} \\
  = &\mathbb{E}_{X}[p(\boldsymbol{y \vert x}) ] 
  \quad \left( \because \mathbb{E}_{X}[f(\boldsymbol{x})] = \int f(\boldsymbol{x})p(\boldsymbol{x}) d\boldsymbol{x} \right)
\end{aligned}
\tag(4)
$$

위 정의는 식(3)의 분자를 잠재변수 $\boldsymbol{x}$ 에 대해 적분하는 형태이다. 따라서 주변 가능도는 $\boldsymbol{x}$ 에 대해 독립적이고, 사후확률 $p(\boldsymbol{x \vert y})$ 가 정규화(normalized)되어 있음을 보장한다.

사과의 예로 보자면 $P(사과) = P(사과 \vert 안동)P(안동) + P(사과 \vert 청송)P(청송) + P(사과 \vert 영주)P(영주) + \cdots$ 로 계산할 수 있고, 베이즈 정리 식은 

$$
\cfrac{P(사과 \vert 안동)P(안동)}{P(사과 \vert 안동)P(안동) + P(사과 \vert 청송)P(청송) + P(사과 \vert 영주)P(영주) + \cdots}
$$

으로 해석할 수 있다.

또한 주변 우도는 우도의 예측값(expected likehood)으로도 볼 수 있다. 위 (4)의 두 번째 줄의 식에 따라 사전확률에 대한 예측(expectation, $\mathbb{E}[\cdot]$) 을 사용하기 때문이다.

베이즈 정리는 가능도로부터 주어진 $\boldsymbol{x, y}$ 간의 관계를 뒤집을 수 있게 하기도 한다. 그래서 종종 베이즈 정리를 'probabilistic inverse' 라고도 한다.

- - -

# 추가 참고 자료

* 다크프로그래머, "[베이지언 확률(Bayesian Probability)](https://darkpgmr.tistory.com/119)"
  * Pure/Naive/Semi-Naive Bayesian
* 다크프로그래머, "[베이즈 정리, ML과 MAP, 그리고 영상처리](https://darkpgmr.tistory.com/62)"
  * 영상처리에 베이즈 정리의 활용