---
title:  "Mathematics for Machine Learning 5.6장: 벡터 연산 - 오차역전파와 자동 미분"
excerpt: "Vector Calculus - Backpropagation and Automatic Differentiation"

categories:
  - Math
  - Math for ML
tags:
  - Math
  - Machine Learning

last_modified_at: 2023-01-27
---

{% include inserted_box.html text="이 내용은 책 <a href='https://mml-book.github.io/book/mml-book.pdf'>Mathematics for machine learning(Marc Peter Deisenroth et al.)</a>을 기반으로 하고 있습니다." %}

# 5.6: Backpropagation and Automatic Differentiation

{% include inserted_box.html text="5.5장은 공식 몇 가지의 나열이기 때문에 포스팅은 생략하겠습니다." %}

머신러닝에선 경사하강법(gradient descent)를 실시해 좋은 모델 파라미터를 찾고자 한다. 이때 모델 파라미터에 대해 목적함수의 그래디언트를 계산한다.

하지만 아무리 chain rule이 편리하고 좋아도, 목적함수의 그래디언트를 직접적으로(수식으로) 계산해서 사용하기엔 비효율적이다. 식 자체가 매우 길 뿐더러 모델이 커질수록 더 복잡해지기 때문이다.

그래서 실제로 심층 신경망 모델을 학습시킬 때는 오차역전파(backpropagation) 알고리즘을 사용해 모델의 파라미터에 대한 오차 함수(error func.)의 그래디언트를 계산한다. 학습 시 앞단에서 뒷단으로 계산해나가는 forward와는 반대 방향으로, 즉 최종단에서 나온 에러를 앞단으로 거꾸로('back') 보내준다('propagation') 하여 오차역전파이다.

## 5.6.1. 심층신경망의 그래디언트

최종 결과물(e.g. 특정 클래스 라벨 등)인 함숫값 $\boldsymbol{y}$는 결국 여러 레벨의 함수의 합성 형태이다. 신경망의 관점에서 보자면 각 함수는 각자의 파라미터를 가진 레이어이고, 함성함수는 레이어를 쌓아 만들었단 뜻이다.

$$
\begin{aligned}
  \boldsymbol{y} &= (f_K \circ f_{K-1} \circ \cdots \circ f_{1})(\boldsymbol{x}) = f_{K}(f_{K-1}(\cdots(f_{1}(\boldsymbol{x}))\cdots)) \\
  f_i &= \sigma_i(\boldsymbol{A}_{i-1} f_{i-1} + b_{i-1}) \ (i = 1, \cdots, K)
\end{aligned}
$$

![image](https://user-images.githubusercontent.com/69252153/214994215-93b70e08-df60-4538-bcd9-05bfee23691c.png)

그럼 이제 모델을 훈련시키기 위해선 파라미터 $\boldsymbol{A}_j$와 $\boldsymbol{b}_j$ ($j = 1, \cdots, K-1$) 에 대한 손실 함수 L의 그래디언트를 구해야 한다.

손실함수를 정답과 예측값 사이의 오차의 제곱이라 하면

$$
\begin{aligned}
  L(\boldsymbol{\theta}) = \lVert \boldsymbol{y} - \boldsymbol{f}_K (\boldsymbol{\theta, x}) \rVert ^2 \\
  (\boldsymbol{\theta} = {\boldsymbol{A_0, b_0, \cdots, A_{K-1}, b_{K-1}}})
\end{aligned}
$$

위 식을 최소로 하는 값을 찾고자 한다.

아래 그림은 L의 그래디언트를 계산하기 위해 backward pass를 하고 있는 모습을 나타내고 있다.

![image](https://user-images.githubusercontent.com/69252153/215030701-9d4bd91e-71f9-4145-9b71-567754d45f23.png)

위 과정을 진행하기 위해, 각 레이어 $j = 1, \cdots, K-1$ 의 파라미터 $\boldsymbol{\theta}_j = \{ \boldsymbol{A_j, b_j} \}$ 에 대한 L의 편미분을 chain rule을 통해 구해보자.

$$
\cfrac{\partial L}{\partial \boldsymbol{\theta}_i} = \cfrac{\partial L}{\partial \boldsymbol{f}_K} \cfrac{\partial \boldsymbol{f}_{K}}{\partial \boldsymbol{f}_{K-1}} \cdots \cfrac{\partial \boldsymbol{f}_{i+2}}{\partial \boldsymbol{f}_{i+1}} \cfrac{\partial \boldsymbol{f}_{i+1}}{\partial \boldsymbol{f}_{i}}
$$

위 식을 보면, 우변의 맨 첫 항은 맨 마지막 레이어에서의 손실함수의 편미분, 맨 마지막 항은 해당 레이어의 파라미터에 대한 이전 레이어의 편미분이다. 그리고 그 사이의 항들은 이전 레이어에 대한 그 다음 레이어의 편미분이다.

위 식과 아래 그림을 엮어 잘 살펴보면, 해당 레이어에서 계산할 값은 우변 맨 마지막 두 항의 chain rule 뿐이란 것을 알 수 있다. 이미 $\cfrac{\partial L}{\partial \boldsymbol{f}_K} \cfrac{\partial \boldsymbol{f}_{K}}{\partial \boldsymbol{f}_{K-1}} \cdots \cfrac{\partial \boldsymbol{f}_{i+2}}{\partial \boldsymbol{f}_{i+1}}$ 까지는 이전 레이어에서 $\cfrac{\partial L}{\partial \boldsymbol{\theta}_{i+1}}$을 구할 때 계산된 내용이다.

![image](https://user-images.githubusercontent.com/69252153/215031514-809d66b5-71f1-4737-af38-2802b54e518a.png)


## 5.6.2. 자동 미분(Automatic Differentiation)
<다시다시!!!>
자동 미분은 수치적(numerically)으로 함수의 정확한 그래디언트를 평가하는 기법의 모음이라 보면 된다. 이를 위해 중간 변수(intermediate)를 사용하기도 하고, chain rule을 사용하기도 한다. 앞장에서 본 오차역전파가 '자동 미분'이라 하는 수치 해석 기법의 일종이다.

자동 미분은 유한 미분을 이용하는 등, Symbolic differentiation, 그래디언트의 Numerical approximation과는 다르다. 





- - -

https://en.wikipedia.org/wiki/Automatic_differentiation