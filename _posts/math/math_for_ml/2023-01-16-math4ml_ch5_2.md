---
title:  "Mathematics for Machine Learning 5.2장: 벡터 연산 - 편미분과 그래디언트"
excerpt: "Vector Calculus - Partial Differentiation and Gradients"

categories:
  - Math
  - Math for ML
tags:
  - Math
  - Machine Learning

last_modified_at: 2023-01-16
---

{% include inserted_box.html text="이 내용은 책 <a href='https://mml-book.github.io/book/mml-book.pdf'>Mathematics for machine learning(Marc Peter Deisenroth et al.)</a>을 기반으로 하고 있습니다." %}

# 5.2: Partial Differentiation and Gradients

5.1장에서는 일변수의 미분을 다루었다면, 이번 장에서는 일반적 형태로 확장한 다변수 함수의 미분을 다룬다. 즉, 함수 $f$는 하나 이상의 변수 $\mathbf{x} \in \mathbb{R}^{n}$를 따르게 된다.

<div id="def-box">
<div class="def-title">Partial Derivatives (편미분)</div>
<p>
n개의 변수 $x_1, \cdots , x_n$의 함수 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}, \mathbf{x} \mapsto f(\mathbf{x})$에 대해 편미분은 아래와 같이 구한다.

$$
\cfrac{\partial f}{\partial x_n} = \lim\limits_{h \rightarrow 0}\cfrac{f(x_1, \cdots , x_{n-1}, \ x_n + h) - f(\mathbf{x})}{h}
$$
</p>
</div>

그래디언트(gradient)는 다변수 함수의 미분의 일반화이다. $\mathbb{x}$에 대한 함수의 그래디언트는 각 변수에 대한 편미분을 모아 행벡터(row vec.)로 모은 것이다. 그래디언트를 Jacobian이라고도 부른다.

<div id="def-box">
<div class="def-title">Gradient</div>
<p>

$$
\triangledown _{\mathbf{x}} f = \mathrm{grad} f = \cfrac{\mathrm{d} f}{\mathrm{d} \mathbf{x}} = \left[ \cfrac{\partial f(\mathbf{x})}{\partial x_1} \quad \cfrac{\partial f(\mathbf{x})}{\partial x_2} \cdots \quad \cfrac{\partial f(\mathbf{x})}{\partial x_n}\right] \in \mathbb{R}^{1 \times n}
$$
</p>
</div>

$\mathbb{R}^{1 \times n}$에서 n은 변수의 개수, 1은 image(상, range, codomain)의 차원이다.

예를 들어 $f(x_1, x_2) = x_1^2 x_2 + x_1x_2^3 \in \mathbb{R}$ 함수에 대한 그래디언트는 아래와 같다.

$$
\cfrac{\mathrm{d} f}{\mathrm{d} \mathbf{x}} 
= \left[ \cfrac{\partial f(x_1, x_2)}{\partial x_1} \quad \cfrac{\partial f(x_1, x_2)}{\partial x_2} \right]
= \left[ 2x_1x_2 + x_2^3 \quad x_1^2 + 3x_1x_2^2 \right]
\in \mathbb{R}^{1 \times 2}
$$

보통 벡터는 열벡터로 나타내는데 반해, 그래디언트는 행벡터로 나타내는데, 이는 그래디언트가 행렬이 되는 벡터 함수(vector-valued func., $\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$)와 동일한 형식을 사용하기 위함이다. 또한 다변수에 chain rule을 적용할 때 그래디언트의 차원을 고려하지 않아도 되기 때문이기도 하다. 그래디언트가 열벡터이기 때문에 5.2.2절에서 볼 수 있듯이 벡터 곱의 형태로 chain rule을 적용할 수도 있다.

## 5.2.1: 편미분의 기본 규칙

* Product rule (곱셈 규칙): $\cfrac{\partial}{\partial \mathbf{x}} \left( f(\mathbf{x})g(\mathbf{x}) \right) = \cfrac{\partial}{\partial \mathbf{x}} g(\mathbf{x}) + f(\mathbf{x}) \cfrac{\partial}{\partial \mathbf{x}}$
* Sum rule (덧셈 규칙): $\cfrac{\partial}{\partial \mathbf{x}} \left( f(\mathbf{x}) + g(\mathbf{x}) \right) = \cfrac{\partial f}{\partial \mathbf{x}} + \cfrac{\partial g}{\partial \mathbf{x}}$
* Chain rule: $\cfrac{\partial}{\partial \mathbf{x}} (f \circ g)(\mathbf{x}) = \cfrac{\partial}{\partial \mathbf{x}} (g (f(\mathbf{x}))) = \cfrac{\partial g}{\partial f} \cfrac{\partial f}{\partial \mathbf{x}}$


## 5.2.2: Chain Rule

함수 $f: \mathbb{R}^{2} \rightarrow \mathbb{R}$이 두 변수 $x_1, x_2$를 가지고 있고 각 변수는 $t$에 대한 함수 $x_1(t), x_2(t)$라고 하자. 이때 $f$의 $t$에 대한 그래디언트 $d$는 아래와 같이 다변수 함수의 chain rule을 통해 구할 수 있다.

$$
\cfrac{\mathrm{d} f}{\mathrm{d} t} 
= \begin{bmatrix} \cfrac{\partial f}{\partial x_1} & \cfrac{\partial f}{\partial x_2} \end{bmatrix}
\begin{bmatrix} \cfrac{\partial x_1(t)}{\partial t} \\ \cfrac{\partial x_2(t)}{\partial t} \end{bmatrix}
= \cfrac{\partial f}{\partial x_1} \cfrac{\partial x_1}{\partial t} + \cfrac{\partial f}{\partial x_2} \cfrac{\partial x_2}{\partial t}
$$

만약 함수 $f(x_1, x_2)$의 변수 $x_1, x_2$도 두 변수 $s, t$에 대한 함수 $x_1(s, t), x_2(s, t)$라면, 행렬곱의 형식으로 그래디언트를 구할 수 있다.

$$
\cfrac{\mathrm{d} f}{\mathrm{d} (s, t)} 
= \cfrac{\partial f}{\partial \mathbf{x}} \cfrac{\partial \mathbf{x}}{\partial (s, t)}
= \begin{bmatrix} \cfrac{\partial f}{\partial x_1} & \cfrac{\partial f}{\partial x_2} \end{bmatrix}
\begin{bmatrix} \cfrac{\partial x_1}{\partial s} & \cfrac{\partial x_1}{\partial t}
\\ \cfrac{\partial x_2}{\partial s} & \cfrac{\partial x_2}{\partial t} \end{bmatrix}
$$

실제 그래디언트를 컴퓨터 프로그램에 적용해 사용할 때 그 값을 체크하기 위해서는, 매우 작은 값 $h$를 택해 유한 미분 근사(finite-defference approximation)를 시켜(편미분을 통해) 이 값과 그래디언트 값을 비교해본다. 에러가 작다면 그래디언트 연산이 옳다고 판단하는 것이다.