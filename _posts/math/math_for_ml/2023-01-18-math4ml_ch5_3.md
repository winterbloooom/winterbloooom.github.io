---
title:  "Mathematics for Machine Learning 5.3장: 벡터 연산 - 벡터값 함수의 그래디언트"
excerpt: "Vector Calculus - Gradients of Vector-Valued Functions"

categories:
  - Math
  - Math for ML
tags:
  - Math
  - Machine Learning

last_modified_at: 2023-01-18
---

# 5.3: Gradients of Vector-Valued Functions

5.1장에서는 일변수 함수의, 5.2장에서는 다변수 함수의 미분을 보았다. 5.3장에서는 벡터에서 벡터로 대응되는 벡터값 함수(벡터장) $\boldsymbol{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 을 살펴본다.

## 벡터장 함수와 편미분

벡터 $\boldsymbol{x} = [x_1, \cdots , x_n]^T \in \mathbb{R}^n$이 있을 때, 벡터값 함수 $\boldsymbol{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$는 아래와 같이 $\mathbb{R}$ 에 매핑하는 함수 $f_i : \mathbb{R}^n \rightarrow \mathbb{R}$의 벡터로 나타낼 수 있다.

$$
\boldsymbol{f}(\boldsymbol{x}) = 
\begin{bmatrix}
  f_1(\boldsymbol{x}) \\ \vdots \\ f_m\boldsymbol{x}
\end{bmatrix}
\in \mathbb{R}^m
$$

그럼 특정 $x_i$에 대한 벡터값 함수의 편미분은

$$
\cfrac{\partial \boldsymbol{f}}{\partial x_i} = 
\begin{bmatrix}
  \lim\limits_{h \rightarrow 0}\cfrac{f_1(x_1, \cdots , x_{i-1}, \ x_i + h, \ x_{i+1}, \cdots, x_n) - f_1(\boldsymbol{x})}{h} \\ 
  \vdots \\ 
  \lim\limits_{h \rightarrow 0}\cfrac{f_m(x_1, \cdots , x_{i-1}, \ x_i + h, \ x_{i+1}, \cdots, x_n) - f_m(\boldsymbol{x})}{h}
\end{bmatrix}
\in \mathbb{R}^m
$$


## 벡터장 함수의 그래디언트, 자코비안

전체 벡터 $\boldsymbol{x}$에 대한 그래디언트는 5.2절에서처럼 각 편미분을 모아 만들 수 있으므로

$$
\cfrac{\mathrm{d} \boldsymbol{f}(\boldsymbol{x})}{\mathrm{d} \boldsymbol{x}} 
= 
\begin{bmatrix}
  \cfrac{\partial \boldsymbol{f}(\boldsymbol{x})}{\partial x_1} &
  \cdots &
  \cfrac{\partial \boldsymbol{f}(\boldsymbol{x})}{\partial x_n}
\end{bmatrix}
=
\begin{bmatrix}
  \cfrac{\partial f_1(\boldsymbol{x})}{\partial x_1} & \cdots & \cfrac{\partial f_1(\boldsymbol{x})}{\partial x_n} \\
  \vdots & & \vdots \\
  \cfrac{\partial f_m(\boldsymbol{x})}{\partial x_1} & \cdots & \cfrac{\partial f_m(\boldsymbol{x})}{\partial x_n} 
\end{bmatrix}
\in \mathbb{R}^{m \times n}
$$


<div id="def-box">
<div class="def-title">Jacobian (자코비안)</div>
<p>

벡터값 함수의 1차 미분을 말한다. $m \times n$ 형태로 나타난다.

$$
\boldsymbol{J} = \triangledown _{\boldsymbol{x}} \boldsymbol{f} = \cfrac{\mathrm{d} \boldsymbol{f}(\boldsymbol{x})}{\mathrm{d} \boldsymbol{x}} 
= \begin{bmatrix}
  \cfrac{\partial f_1(\boldsymbol{x})}{\partial x_1} & \cdots & \cfrac{\partial f_1(\boldsymbol{x})}{\partial x_n} \\
  \vdots & & \vdots \\
  \cfrac{\partial f_m(\boldsymbol{x})}{\partial x_1} & \cdots & \cfrac{\partial f_m(\boldsymbol{x})}{\partial x_n} 
\end{bmatrix}
$$

이때 $\boldsymbol{x} = [x_1 \ \cdots x_n]^T$ 이고, $J(i, j) = \cfrac{\partial f_i}{\partial x_j}$이다.
</p>
</div>

5.2절에서 보았던 다변수 함수의 미분이 위 자코비안의 한 형태였던 것이다.

위처럼 함수 $\boldsymbol{f}$의 요소를 행에, 변수 $\boldsymbol{x}$의 요소를 열에 적는 형태를 numerator layout이라 하며, 그 반대로 적은 denominator layout이라고 한다.

## 자코비안의 쓰임

자코비안의 행렬식(determinant)는 scaling 값을 나타낸다.

행렬의 행렬식은 평행사변형의 넓이이다. 변수의 변형(transformation)으로 인한 scaling factor도 행렬식으로 나타낼 수 있는 것이다.

예를 들어 아래와 같이 b에서 c로 변하는 transformation이 있다고 하자. 이 매핑은 선형적이고, 이 매핑의 행렬식의 절댓값이 scaling factor가 된다는 뜻이다.

![image](https://user-images.githubusercontent.com/69252153/213151802-9bfee57a-4f09-4769-a56d-1f8d1cab2948.png)

위와 같은 매핑을 찾기 위한 접근법이 두 가지가 있는데, 하나는 매핑이 선형적이란 점을 이용하는 선형대수적(linear algegra) 접근, 또 하나는 편미분을 이용해 비선형적 변형까지도 나타낼 수 있는 접근이다.

첫 번째 방식은 b의 기저를 c의 기저로 변환하는 transformation matrix를 찾는 문제로 귀결된다. 그리고 이 행렬의 행렬식이 우리가 찾는 scaling factor다. b로 그려지는 사각형의 넓이의 'scaling factor'배가 바로 c로 그려지는 사각형의 넓이다.

두 번째 방식을 조금 더 주의깊게 살펴보자. 함수 $\boldsymbol{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ 가 transformation을 수행한다고 할 때(위 그림의 경우 b에서 c로), $\boldsymbol{f}$ 에 의한 변형이 일어났을 때 넓이(부피)가 얼마나 변하는지에 관한 매핑을 찾는단 뜻이다. 그 말인 즉, $\boldsymbol{x}$가 약간 변할 때 $\boldsymbol{f}$가 얼마나 변하는지를 찾는 문제가 된다. 이는 곧 자코비안 행렬이기도 하다.

$$
\begin{aligned}
  y_1 &= -2x_1 + x_2 \\
  y_2 &= x_1 + x_2
\end{aligned}
$$

위와 같이 함수 관계를 나타낼 수 있고, 편미분 또한 구할 수 있다. 그럼 자코비안은

$$
\boldsymbol{J}
= \begin{bmatrix}
  \cfrac{\partial y_1}{\partial x_1} & \cfrac{\partial y_1}{\partial x_2} \\
  \cfrac{\partial y_2}{\partial x_1} & \cfrac{\partial y_2}{\partial x_2} \\
\end{bmatrix}
= \begin{bmatrix}
  -2 & 1 \\ 1 & 1
\end{bmatrix}
$$

가 된다. 이것이 바로 우리가 구하려는 transform이다.

만약 비선형적 변환이라면, 자코비안은 비선형적 변환을 국소적으로(locally) 선형 변형으로 근사시킨다. 자코비안의 행렬식의 절댓값 $\lvert \det (\boldsymbol{J}) \rvert$가 변환 시의 scaling factor이다.

뒤에 이어지즌 6.7절에서는 random variables와 확률 분포의 transformation에 자코비안을 사용하는 예를 볼 것이다.(infinite perturbation analysis)

## 편미분의 차원 및 그래디언트 시각화

![image](https://user-images.githubusercontent.com/69252153/213154888-b2f0bbd7-e19c-46a9-97bd-2ab84f73f487.png)

위 그림은 편미분의 dimensionality(차원)을 간략화한 그림이다. 1차원에서 1차원으로 가는 함수에 대해서 그래디언트는 스칼라(좌상단 주황색 박스)이고, 다차원에서 다차원으로 가는 벡터값 함수의 그래디언트는 행렬이다(우하단 주황색 박스).

벡터에 대한 행렬의 그래디언트 계산을 보는 두 가지 시각이 있을 수 있다. 첫 번째는 아래 그림처럼 편미분을 합쳐(collating) 자코비안 텐서로 만드는 방법이다.

![image](https://user-images.githubusercontent.com/69252153/213159456-af3e960a-6b9f-42ba-9714-cdff31416e2e.png)

두 번째는 행렬을 벡터로 flatten한 뒤, 자코비안 행렬을 계산해 텐서로 변환하는 방식이다.

![image](https://user-images.githubusercontent.com/69252153/213159628-dc7f4ea7-2f88-40f2-8d32-8b892d221427.png)


## 선형 모델에서 최소제곱 오차의 그래디언트

그래디언트를 사용하는 예로, 선형 모델(liear model)의 최소제곱 오차(least-squares loss)의 그래디언트를 계산하는 과정을 살펴보자.

아래와 같은 선형 모델이 있다고 하자. $\boldsymbol{\theta} \in \mathbb{R}^D$ 는 파라미터 벡터이고, $\boldsymbol{\Phi} \in \mathbb{R}^{N \times D}$ 는 입력 피처(특징), $\boldsymbol{y} \in \mathbb{R}^N$ 은 정답(observation) 값이다.

$$
\boldsymbol{y} = \boldsymbol{\Phi \theta}
$$

해당 모델의 목표는 오차를 최소화하는 파라미터를 찾는 것이 될 테다. 오차(error)와 최소제곱 손실함수(loss func.)를 각각 아래와 같이 정의한다고 하자.

$$
\begin{aligned}
  \boldsymbol{e}(\boldsymbol{\theta}) &:= \boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{\theta} \\
L(\boldsymbol{e}) &:= || \boldsymbol{e} || ^2
\end{aligned}

$$

손실함수를 최소로 하는 파라미터를 구해야 하므로, $\cfrac{\partial L}{\partial \boldsymbol{\theta}}$ 를 찾아 이 값이 최소가 되도록 조정해야 할 것이다. 편미분을 구하는 것은 chain rule을 이용하면 된다.

$$
\begin{aligned}
  \cfrac{\partial L}{\partial \boldsymbol{\theta}} &\in \mathbb{R}^{1 \times D} \\
  &= \cfrac{\partial L}{\partial \boldsymbol{e}} \cfrac{\partial \boldsymbol{e}}{\partial \boldsymbol{\theta}}
\end{aligned}
$$

즉 d번째 요소에 대해서는

$$
\cfrac{\partial L}{\partial \boldsymbol{\theta}} [1, d] 
= \sum\limits_{n=1}^N \cfrac{\partial L}{\partial \boldsymbol{e}} [n] \cfrac{\partial \boldsymbol{e}}{\partial \boldsymbol{\theta}} [n, d]
$$

로 구한다. 참고로 이는 아인슈타인 합을 이용해 `dLdtheta = np.einsum('n, nd', dLde, dedtheta)`의 NumPy 코드로 나타낼 수 있다. (einsum 설명: [Einsum에 대해 간략한 정리](https://ita9naiwa.github.io/numeric%20calculation/2018/11/10/Einsum.html))

앞선 chain rule의 우변 첫 번째 항은

$$
\cfrac{\partial L}{\partial \boldsymbol{e}}  = 2\boldsymbol{e}^T \in \mathbb{R}^{1 \times N} \quad (\because || \boldsymbol{e} || ^2 = \boldsymbol{e}^T\boldsymbol{e})
$$

두 번째 항은 

$$
\cfrac{\partial \boldsymbol{e}}{\partial \boldsymbol{\theta}} = -\boldsymbol{\Phi} \in \mathbb{R}^{N \times D}
$$

이 둘을 합치면

$$
\cfrac{\partial L}{\partial \boldsymbol{\theta}} = -2\boldsymbol{e}^T \boldsymbol{\Phi} = -2 (\boldsymbol{y}^T - \boldsymbol{\theta}^T \boldsymbol{\Phi}^T)\boldsymbol{\Phi} \in \mathbb{R}^{1 \times D}
$$

가 된다. 물론 직접적으로 $L_2 (\boldsymbol{\theta}) := \lVert \boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{\theta}  \rVert^2 = (\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{\theta} )^T(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{\theta} )$로 바로 구할 수도 있다.